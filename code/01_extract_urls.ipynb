{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###\n",
    "# @Author             : Monserrat LÃ³pez\n",
    "# @Date               : 2025-01-10\n",
    "# @Last Modified Date : 2025-04-21\n",
    "# @Description        : Code for extracting URLs from XML sitemaps, parsing them into structured components and filter for EU countries\n",
    "# @Note               : This script is intended for academic research purposes only. Do not use for commercial purposes\n",
    "#                       Some original raw data used during development is not included in this repository for confidentiality reasons."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:21.620515Z",
     "start_time": "2025-04-23T12:17:21.612951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Required libraries \n",
    "import xml.etree.ElementTree as ET  # For parsing XML sitemaps\n",
    "import os                           # For file system operations\n",
    "import csv                          # For writing CSV outputs\n",
    "import pandas as pd                 # For structured data handling\n",
    "import regex as re                  # Extended regular expressions for robust pattern matching"
   ],
   "id": "3305a968b35b2f4a",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse the sitemap XMLs to extract all datacenter URLs.",
   "id": "ecb491e097553208"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:22.545723Z",
     "start_time": "2025-04-23T12:17:22.543110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define input and output paths\n",
    "input_folder = \"../input/DCMap\"                      \n",
    "output_file = \"../output/01datacenter_urls.csv\" "
   ],
   "id": "5f340e98d5da06d2",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:22.934177Z",
     "start_time": "2025-04-23T12:17:22.930322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define helper function to extract <loc> tags from XML sitemap files\n",
    "def extract_urls_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Parses an XML sitemap and extracts all <loc> tags containing data center URLs.\n",
    "    Returns a list of URLs or an empty list if the file is unreadable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        tree = ET.parse(file_path)\n",
    "        root = tree.getroot()\n",
    "        namespace = {'ns': 'http://www.sitemaps.org/schemas/sitemap/0.9'}\n",
    "        \n",
    "        # Extract text content of each <loc> tag\n",
    "        urls = [url.text.strip() for url in root.findall(\".//ns:loc\", namespace)]\n",
    "        return urls\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return []"
   ],
   "id": "868f46878daa73f1",
   "outputs": [],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:23.434669Z",
     "start_time": "2025-04-23T12:17:23.432036Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Collect all .xml sitemap files from the input folder\n",
    "xml_files = [os.path.join(input_folder, file) for file in os.listdir(input_folder) if file.endswith(\".xml\")]"
   ],
   "id": "2cfa1e03e22d376a",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:23.813597Z",
     "start_time": "2025-04-23T12:17:23.770222Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract all URLs from each XML file\n",
    "all_urls = []\n",
    "for xml_file in xml_files:\n",
    "    print(f\"Processing file: {xml_file}\")\n",
    "    urls = extract_urls_from_file(xml_file)\n",
    "    all_urls.extend(urls)"
   ],
   "id": "966fbce0c776881d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ../input/DCMap/dcs_3.xml\n",
      "Processing file: ../input/DCMap/dcs_2.xml\n",
      "Processing file: ../input/DCMap/dcs_1.xml\n",
      "Processing file: ../input/DCMap/dcs_5.xml\n",
      "Processing file: ../input/DCMap/dcs_4.xml\n",
      "Processing file: ../input/DCMap/dcs_6.xml\n",
      "Processing file: ../input/DCMap/dcs_7.xml\n",
      "Processing file: ../input/DCMap/dcs_9.xml\n",
      "Processing file: ../input/DCMap/dcs_8.xml\n",
      "Processing file: ../input/DCMap/dcs_10.xml\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:30:05.589428Z",
     "start_time": "2025-04-23T12:30:05.571112Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save the list of extracted URLs to a CSV file\n",
    "with open(output_file, mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"Datacenter URL\"])  # CSV header\n",
    "    for url in all_urls:\n",
    "        writer.writerow([url])\n",
    "\n",
    "print(f\"Extracted {len(all_urls)} URLs and saved them to '{output_file}'\")"
   ],
   "id": "90b3aab268170fd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 9850 URLs and saved them to '../output/01datacenter_urls.csv'\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parse URLs to extract structured fields (country, state, city, name).",
   "id": "9a039e0aa61199cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:27.653073Z",
     "start_time": "2025-04-23T12:17:27.646239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the saved URLs for further processing\n",
    "input_folder = \"../output/01datacenter_urls.csv\"    "
   ],
   "id": "17fd2c51e0d7a167",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:27.930825Z",
     "start_time": "2025-04-23T12:17:27.892147Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the list of URLs into a pandas DataFrame\n",
    "data = pd.read_csv(input_folder, encoding='utf-8')"
   ],
   "id": "cd0cd879c6cb706",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:28.260331Z",
     "start_time": "2025-04-23T12:17:28.255152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define a function to parse structured fields from each URL\n",
    "def parse_url(url):\n",
    "    \"\"\"\n",
    "    Parses a DataCenterMap URL to extract structured fields:\n",
    "    - Country\n",
    "    - State (optional)\n",
    "    - City\n",
    "    - DataCenter name\n",
    "\n",
    "    Returns a tuple with the extracted components.\n",
    "    \"\"\"\n",
    "    pattern = r\"https://www\\.datacentermap\\.com/(?P<country>[^/]+)/(?:(?P<state>[^/]+)/)?(?P<city>[^/]+)/(?P<datacenter>[^/]+)/?\"\n",
    "    match = re.match(pattern, url)\n",
    "    if match:\n",
    "        return match.group(\"country\", \"state\", \"city\", \"datacenter\")\n",
    "    else:\n",
    "        return None, None, None, None"
   ],
   "id": "ac47ea336fcfffdf",
   "outputs": [],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:28.920005Z",
     "start_time": "2025-04-23T12:17:28.845927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Parse each URL and extract structured metadata\n",
    "structured_data = []\n",
    "with open(input_folder, mode=\"r\", encoding=\"utf-8\") as infile:\n",
    "    reader = csv.reader(infile)\n",
    "    next(reader)  # Skip header row\n",
    "    for row in reader:\n",
    "        url = row[0]\n",
    "        country, state, city, datacenter = parse_url(url)\n",
    "        structured_data.append([url, country, state, city, datacenter])"
   ],
   "id": "3efac8b4d4d6c143",
   "outputs": [],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:29.715551Z",
     "start_time": "2025-04-23T12:17:29.707968Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(structured_data, columns=[\"Datacenter URL\", \"Country\", \"State\", \"City\", \"Datacenter Name\"])"
   ],
   "id": "dddc23d7ee252f80",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Filter and clean the dataset to retain only EU-based data centers.\n",
   "id": "b243b6f7844cd64c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:17:33.557603Z",
     "start_time": "2025-04-23T12:17:33.536168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "OUTPUT_EU_CSV = \"../output/02european_datacenters.csv\"\n",
    "\n",
    "# Define EU countries (URL-friendly naming)\n",
    "eu_countries = {\n",
    "    'austria', 'belgium', 'bulgaria', 'croatia', 'cyprus', 'czech-republic', 'denmark',\n",
    "    'estonia', 'finland', 'france', 'germany', 'greece', 'hungary', 'ireland', 'italy',\n",
    "    'latvia', 'lithuania', 'luxembourg', 'malta', 'the-netherlands', 'poland', 'portugal',\n",
    "    'romania', 'slovakia', 'slovenia', 'spain', 'sweden'\n",
    "}\n",
    "\n",
    "# Filter for EU27 and clean up\n",
    "df_eu = df[df[\"Country\"].isin(eu_countries)].drop_duplicates()\n",
    "df_eu = df_eu.drop(columns=[\"State\"], errors=\"ignore\")\n",
    "\n",
    "# Save filtered dataset\n",
    "df_eu.to_csv(OUTPUT_EU_CSV, index=False, encoding=\"utf-8\")\n",
    "print(f\"[DONE] Saved filtered EU data to '{OUTPUT_EU_CSV}' with {len(df_eu)} entries.\")"
   ],
   "id": "1df2af48496985db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DONE] Saved filtered EU data to '../output/02european_datacenters.csv' with 1795 entries.\n"
     ]
    }
   ],
   "execution_count": 37
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
