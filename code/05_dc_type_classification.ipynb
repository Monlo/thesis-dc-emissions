{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###\n",
    "# @Author             : Monserrat LÃ³pez\n",
    "# @Date               : 2025-03-01\n",
    "# @Last Modified Date : 2025-04-25\n",
    "# @Description        : Data cleaning, classification, and imputation pipeline for EU data centers.\n",
    "#                       Includes missing value analysis, rule-based + NLP classification into hyperscale, colocation, and enterprise types,\n",
    "#                       whitespace imputation strategies, and export of the cleaned dataset for energy modeling.\n",
    "# @Note               : This script is intended for academic research purposes only. \n",
    "#                       Some original raw data collected during the research is not included in this repository for confidentiality reasons.\n",
    "###"
   ],
   "id": "56daaf5a9a62d666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:46.882821Z",
     "start_time": "2025-04-27T11:19:46.875227Z"
    }
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": 39,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string"
   ],
   "id": "64ba67c9054f3066"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Missing value analysis",
   "id": "ccc71bfe6af9007b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:47.206077Z",
     "start_time": "2025-04-27T11:19:47.172177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set the visualization style\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('../output/08_DC_EU27.csv')\n",
    "\n",
    "# Display basic information\n",
    "print(f\"Total data centers: {len(df)}\")\n",
    "print(f\"Countries represented: {df['country_normalized'].nunique()}\")\n",
    "\n",
    "# Select the relevant columns for missing value analysis\n",
    "relevant_vars = [\n",
    "    'pue_estimate',\n",
    "    'power_built_out_mw',\n",
    "    'live_power_mw',\n",
    "    'whitespace_sqm',\n",
    "    'building_size_sqm',\n",
    "    'tier_level',\n",
    "    'latitude',\n",
    "    'longitude'\n",
    "]\n",
    "\n",
    "# Summary of missing values\n",
    "missing_summary = df[relevant_vars].isnull().sum().to_frame(name='missing_count')\n",
    "missing_summary['total'] = len(df)\n",
    "missing_summary['missing_percent'] = (missing_summary['missing_count'] / missing_summary['total']) * 100\n",
    "print(\"Missing values summary:\")\n",
    "print(missing_summary)\n"
   ],
   "id": "26be66092ac161fe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data centers: 1600\n",
      "Countries represented: 27\n",
      "Missing values summary:\n",
      "                    missing_count  total  missing_percent\n",
      "pue_estimate                 1513   1600          94.5625\n",
      "power_built_out_mw           1065   1600          66.5625\n",
      "live_power_mw                1587   1600          99.1875\n",
      "whitespace_sqm                976   1600          61.0000\n",
      "building_size_sqm            1416   1600          88.5000\n",
      "tier_level                   1166   1600          72.8750\n",
      "latitude                        0   1600           0.0000\n",
      "longitude                       0   1600           0.0000\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:47.853021Z",
     "start_time": "2025-04-27T11:19:47.325508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Distribution of data centers by country\n",
    "plt.figure(figsize=(14, 8))\n",
    "country_counts = df['country_normalized'].value_counts()\n",
    "sns.barplot(x=country_counts.index, y=country_counts.values)\n",
    "plt.title('Number of Data Centers by Country')\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/datacenter_count_by_country.png', dpi=300)\n",
    "plt.close()"
   ],
   "id": "99cb7e71e311cdad",
   "outputs": [],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:47.877474Z",
     "start_time": "2025-04-27T11:19:47.870260Z"
    }
   },
   "cell_type": "code",
   "source": "country_counts",
   "id": "25dbf681fb43a227",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_normalized\n",
       "Germany           341\n",
       "France            221\n",
       "Netherlands       148\n",
       "Italy             129\n",
       "Spain             111\n",
       "Sweden             76\n",
       "Ireland            74\n",
       "Poland             74\n",
       "Romania            53\n",
       "Denmark            43\n",
       "Austria            41\n",
       "Belgium            37\n",
       "Czech Republic     36\n",
       "Portugal           31\n",
       "Finland            31\n",
       "Bulgaria           23\n",
       "Slovenia           17\n",
       "Croatia            16\n",
       "Lithuania          16\n",
       "Latvia             15\n",
       "Greece             15\n",
       "Cyprus             11\n",
       "Hungary            10\n",
       "Luxembourg          9\n",
       "Slovakia            9\n",
       "Estonia             8\n",
       "Malta               5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data Preprocessing",
   "id": "3381ff1e8ad0c513"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:47.931656Z",
     "start_time": "2025-04-27T11:19:47.925168Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Prioritize 'live_power_mw' over 'power_built_out_mw' for estimating IT power capacity\n",
    "df['power_capacity_mw'] = df['live_power_mw'].fillna(df['power_built_out_mw'])\n",
    "\n",
    "# Prioritize 'whitespace_sqm' over 'building_size_sqm' for usable building area\n",
    "df['building_area_sqm'] = df['whitespace_sqm'].fillna(df['building_size_sqm'])\n",
    "\n",
    "# Report remaining missing values\n",
    "missing_power = df['power_capacity_mw'].isnull().sum()\n",
    "missing_area = df['building_area_sqm'].isnull().sum()\n",
    "total = len(df)\n",
    "\n",
    "print(f\"Data centers with missing power capacity: {missing_power} ({missing_power / total * 100:.2f}%)\")\n",
    "print(f\"Data centers with missing building area: {missing_area} ({missing_area / total * 100:.2f}%)\")"
   ],
   "id": "62e2fd9354adacd5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data centers with missing power capacity: 1065 (66.56%)\n",
      "Data centers with missing building area: 929 (58.06%)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### NLP Model for categorizing data centres",
   "id": "508d4ed38f5949eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:48.363471Z",
     "start_time": "2025-04-27T11:19:48.358516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download necessary NLTK resources (uncomment first time)\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Initialize NLTK components\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Prepare a seed dataset with known classifications\n",
    "# Define keywords and operators associated with each type\n",
    "dc_types = {\n",
    "    'hyperscale': {\n",
    "        'keywords': [\n",
    "            'hyperscale', 'hyperscaler', 'global cloud', 'cloud-scale', \n",
    "            'hyperscale facility', 'hyperscale infrastructure',\n",
    "            'hyperscale data center', 'hyperscale data centre'\n",
    "        ],\n",
    "        'operators': [\n",
    "            'amazon', 'aws', 'google', 'microsoft', 'azure', 'meta', 'facebook',\n",
    "            'apple', 'alibaba', 'tencent', 'oracle', 'ibm', 'baidu', 'huawei', \n",
    "            'ovh', 'switch', 'cloudflare', 'salesforce'\n",
    "        ]\n",
    "    },\n",
    "    'colocation': {\n",
    "        'keywords': [\n",
    "            'colocation', 'co-location', 'colo', 'carrier-neutral', \n",
    "            'multi-tenant', 'wholesale', 'neutral facility'\n",
    "        ],\n",
    "        'operators': [\n",
    "            'equinix', 'digital realty', 'ntt', 'cyrusone', 'qts', 'coresite',\n",
    "            'global switch', 'interxion', 'telehouse', 'colt', 'edgeconnex', \n",
    "            'vantage', 'iron mountain', 'atman', 'data4', 'dataplex', 'nexcenter'\n",
    "        ]\n",
    "    },\n",
    "    'enterprise': {\n",
    "        'keywords': [\n",
    "            'enterprise', 'private', 'in-house', 'on-premise', 'corporate', \n",
    "            'dedicated data center', 'dedicated facility'\n",
    "        ],\n",
    "        'operators': []  # Still leave blank\n",
    "    }\n",
    "}\n",
    "\n"
   ],
   "id": "2b12e83e32e55337",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:48.747591Z",
     "start_time": "2025-04-27T11:19:48.546137Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Clean description \n",
    "def clean_description(text):\n",
    "    if pd.isna(text) or not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Remove common headers and noisy sections\n",
    "    patterns_to_remove = [\n",
    "        r'Data Centers[A-Za-z]+\\n',        # \"Data CentersSpainBarcelona\"\n",
    "        r'Visit Website.*?\\n',             # \"Visit Website Overview Specs ...\"\n",
    "        r'Overview.*?Request Quote\\n',     # Sections\n",
    "        r'Suites.*?Public Cloud Servers\\n', # Lists of services\n",
    "        r'Colt.*?Data Centre Services.*?\\n', # Marketing phrases\n",
    "        r'AtlasEdge.*?Data Sheet.*?\\n',\n",
    "        r'ISO\\s*\\d{4,5}',                  # Certifications\n",
    "        r'Tier\\s*\\d+',                     # Tier info\n",
    "        r'(?i)EVENTS.*',                   # Timeline metadata\n",
    "        r'\\bCompany name:.*',              # Company name info\n",
    "        r'(?i)(colt|atlasedge) [a-z0-9\\s,.-]+ acquired',  # Acquisition notes\n",
    "    ]\n",
    "\n",
    "    for pattern in patterns_to_remove:\n",
    "        text = re.sub(pattern, '', text, flags=re.DOTALL)\n",
    "\n",
    "    # Remove emails and URLs\n",
    "    text = re.sub(r'\\S+@\\S+', '', text)                     # Emails\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)              # URLs\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()                # Extra whitespace\n",
    "    return text\n",
    "\n",
    "df['cleaned_description'] = df['description'].apply(clean_description)"
   ],
   "id": "7ec8e7ec0cc19ba8",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:48.791090Z",
     "start_time": "2025-04-27T11:19:48.760746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "### Define and apply rule-based classification\n",
    "def classify_dc_by_rules(row):\n",
    "    description = str(row.get('cleaned_description', '')).lower()\n",
    "    operator = str(row.get('operator_name', '')).lower()\n",
    "    datacenter_name = str(row.get('datacentername', '')).lower()\n",
    "    combined_text = f\"{description} {operator} {datacenter_name}\"\n",
    "\n",
    "    # 1. Hyperscale logic: Require BOTH strong keyword OR strong operator\n",
    "    hyperscale_match = any(kw in combined_text for kw in dc_types['hyperscale']['keywords'])\n",
    "    hyperscale_operator = any(op in operator for op in dc_types['hyperscale']['operators'])\n",
    "\n",
    "    if hyperscale_operator or hyperscale_match:\n",
    "        return 'hyperscale'\n",
    "\n",
    "    # 2. Colocation logic: Same (keyword or operator)\n",
    "    colocation_match = any(kw in combined_text for kw in dc_types['colocation']['keywords'])\n",
    "    colocation_operator = any(op in operator for op in dc_types['colocation']['operators'])\n",
    "\n",
    "    if colocation_operator or colocation_match:\n",
    "        return 'colocation'\n",
    "\n",
    "    # 3. Enterprise logic: Keywords only (no operator matching needed)\n",
    "    enterprise_match = any(kw in combined_text for kw in dc_types['enterprise']['keywords'])\n",
    "\n",
    "    if enterprise_match:\n",
    "        return 'enterprise'\n",
    "\n",
    "    # 4. Fallback: Assume enterprise if nothing else matches\n",
    "    return 'enterprise'\n",
    "\n",
    "df['dc_type_rule'] = df.apply(classify_dc_by_rules, axis=1)\n",
    "print(\"\\nInitial Rule-Based Classification:\")\n",
    "print(df['dc_type_rule'].value_counts())"
   ],
   "id": "fc18d4a31cca5bd3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Initial Rule-Based Classification:\n",
      "dc_type_rule\n",
      "enterprise    795\n",
      "colocation    670\n",
      "hyperscale    135\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:49.923809Z",
     "start_time": "2025-04-27T11:19:48.911261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Preprocess cleaned text for NLP model\n",
    "def preprocess_text(text):\n",
    "    if pd.isna(text) or text == \"\":\n",
    "        return \"\"\n",
    "    text = text.lower()\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "# Combine cleaned text fields\n",
    "df['combined_text'] = (\n",
    "    df['cleaned_description'].fillna('') + ' ' +\n",
    "    df['datacentername'].fillna('') + ' ' +\n",
    "    df['operator_name'].fillna('')\n",
    ")\n",
    "\n",
    "df['processed_text'] = df['combined_text'].apply(preprocess_text)\n",
    "\n",
    "# Engineered keyword flags\n",
    "df['has_datacenter_word'] = df['combined_text'].str.contains('data center|datacenter|server|rack', case=False, regex=True).astype(int)\n",
    "df['has_cloud_word'] = df['combined_text'].str.contains('cloud|aws|azure|gcp', case=False, regex=True).astype(int)\n",
    "df['has_colo_word'] = df['combined_text'].str.contains('colo|wholesale|multi-tenant', case=False, regex=True).astype(int)\n",
    "df['has_enterprise_word'] = df['combined_text'].str.contains('enterprise|private|corporate', case=False, regex=True).astype(int)"
   ],
   "id": "b091451a862f3583",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:50.623407Z",
     "start_time": "2025-04-27T11:19:49.932208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train NLP classification model\n",
    "# Filter to rows with non-empty processed text\n",
    "df_with_text = df[df['processed_text'] != ''].copy()\n",
    "\n",
    "# Use TF-IDF to convert text to numerical features\n",
    "tfidf = TfidfVectorizer(max_features=1000, min_df=5, max_df=0.7)\n",
    "text_features = tfidf.fit_transform(df_with_text['processed_text']).toarray()\n",
    "\n",
    "# Combine with numerical features\n",
    "numerical_features = df_with_text[\n",
    "    ['has_datacenter_word', 'has_cloud_word', 'has_colo_word', 'has_enterprise_word']].values\n",
    "X = np.hstack((text_features, numerical_features))\n",
    "y = df_with_text['dc_type_rule']  # Using rule-based classification as target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = rf_model.predict(X_test)\n",
    "print(\"\\nNLP Model Performance:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=np.unique(y), yticklabels=np.unique(y))\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix for Data Center Type Classification')\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/dc_type_confusion_matrix.png', dpi=300)\n",
    "plt.close()"
   ],
   "id": "9d1eee785c63465f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NLP Model Performance:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  colocation       0.98      0.97      0.97       129\n",
      "  enterprise       0.96      1.00      0.98       163\n",
      "  hyperscale       0.96      0.79      0.86        28\n",
      "\n",
      "    accuracy                           0.97       320\n",
      "   macro avg       0.97      0.92      0.94       320\n",
      "weighted avg       0.97      0.97      0.97       320\n",
      "\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:50.987533Z",
     "start_time": "2025-04-27T11:19:50.650839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply model to all data centers with text\n",
    "all_with_text = df[df['processed_text'] != ''].copy()\n",
    "all_text_features = tfidf.transform(all_with_text['processed_text']).toarray()\n",
    "all_numerical_features = all_with_text[\n",
    "    ['has_datacenter_word', 'has_cloud_word', 'has_colo_word', 'has_enterprise_word']].values\n",
    "all_X = np.hstack((all_text_features, all_numerical_features))\n",
    "\n",
    "# Predict\n",
    "all_with_text['dc_type_nlp'] = rf_model.predict(all_X)\n",
    "\n",
    "# Combine rule-based and NLP predictions\n",
    "df['dc_type_nlp'] = all_with_text['dc_type_nlp']\n",
    "\n",
    "# For records without text, use rule-based classification\n",
    "df['dc_type_final'] = df['dc_type_nlp'].fillna(df['dc_type_rule'])\n",
    "\n",
    "# Count the number of data centers in each category\n",
    "print(\"\\nFinal Classification:\")\n",
    "print(df['dc_type_final'].value_counts())\n",
    "\n",
    "# Visualize the classification results\n",
    "plt.figure(figsize=(10, 6))\n",
    "df['dc_type_final'].value_counts().plot(kind='bar', color='skyblue')\n",
    "plt.title('Data Center Classification Results')\n",
    "plt.xlabel('Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.savefig('../figures/dc_type_distribution.png', dpi=300)\n",
    "plt.close()"
   ],
   "id": "30220565b77fb659",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Classification:\n",
      "dc_type_final\n",
      "enterprise    801\n",
      "colocation    669\n",
      "hyperscale    130\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Impute whitespace",
   "id": "edabdffbc5fa0a87"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:51.166226Z",
     "start_time": "2025-04-27T11:19:51.159738Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create a working column\n",
    "df['ws_final'] = df['whitespace_sqm']\n",
    "\n",
    "# --- Imputation 1: Impute from building size (50%)\n",
    "mask_building = df['ws_final'].isna() & df['building_size_sqm'].notna()\n",
    "df.loc[mask_building, 'ws_final'] = df.loc[mask_building, 'building_size_sqm'] * 0.5\n",
    "print(f\"Imputed from building_size_sqm: {mask_building.sum()} rows\")"
   ],
   "id": "998ac2da75a92b6c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imputed from building_size_sqm: 47 rows\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:52.090504Z",
     "start_time": "2025-04-27T11:19:52.077247Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Compute medians per data centre type\n",
    "type_medians = (\n",
    "    df[df['whitespace_sqm'].notna()]\n",
    "    .groupby('dc_type_final')['whitespace_sqm']\n",
    "    .median()\n",
    "    .to_dict()\n",
    ")\n",
    "print(\"Medians per data centre type:\", type_medians)\n",
    "\n",
    "# Impute using type-specific medians\n",
    "mask_median = df['ws_final'].isna() & df['dc_type_final'].notna()\n",
    "for t, median_val in type_medians.items():\n",
    "    t_mask = mask_median & (df['dc_type_final'] == t)\n",
    "    df.loc[t_mask, 'ws_final'] = median_val\n",
    "    print(f\"Imputed from type-specific median ({t}): {t_mask.sum()} rows\")"
   ],
   "id": "3cf3fceb7f2c202d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Medians per data centre type: {'colocation': 1500.0, 'enterprise': 700.0, 'hyperscale': 7250.0}\n",
      "Imputed from type-specific median (colocation): 292 rows\n",
      "Imputed from type-specific median (enterprise): 531 rows\n",
      "Imputed from type-specific median (hyperscale): 106 rows\n"
     ]
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:52.813288Z",
     "start_time": "2025-04-27T11:19:52.810599Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Assign the imputed whitespace values to the main column\n",
    "df['whitespace_sqm'] = df['ws_final']"
   ],
   "id": "6a11c3540ae7db95",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:53.350624Z",
     "start_time": "2025-04-27T11:19:53.345240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Track the source of each imputation BEFORE assigning whitespace_sqm\n",
    "df['whitespace_source'] = 'original'\n",
    "df.loc[mask_building, 'whitespace_source'] = 'from_building_size'\n",
    "df.loc[mask_median, 'whitespace_source'] = 'from_type_median'"
   ],
   "id": "7b46aa5706b11772",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:53.876718Z",
     "start_time": "2025-04-27T11:19:53.865059Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Summary statistics\n",
    "summary = df.groupby('whitespace_source')['whitespace_sqm'].describe().round(1)\n",
    "print(\"\\nImputation summary by source:\")\n",
    "print(summary)"
   ],
   "id": "3a96f0c151973e53",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Imputation summary by source:\n",
      "                    count     mean      std    min     25%     50%      75%  \\\n",
      "whitespace_source                                                             \n",
      "from_building_size   47.0  12376.6  22051.0   45.0  1270.0  5000.0  10980.5   \n",
      "from_type_median    929.0   1698.8   2025.6  700.0   700.0   700.0   1500.0   \n",
      "original            624.0   3593.3   8913.8   30.0   400.0  1069.5   3171.5   \n",
      "\n",
      "                         max  \n",
      "whitespace_source             \n",
      "from_building_size  108371.5  \n",
      "from_type_median      7250.0  \n",
      "original            110000.0  \n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:54.723307Z",
     "start_time": "2025-04-27T11:19:54.629804Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Export cleaned, validated and categorized EU data centers for modeling\n",
    "df.to_csv(\"../output/09_classified_DC_EU27.csv\", index=False, encoding=\"utf-8\")\n",
    "print(f\"Final cleaned dataset saved with {len(df)} records.\")"
   ],
   "id": "30f8c6503c379d63",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cleaned dataset saved with 1600 records.\n"
     ]
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:19:59.695846Z",
     "start_time": "2025-04-27T11:19:59.688875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "missing_whitespace = df['whitespace_sqm'].isna().sum()\n",
    "total = len(df)\n",
    "print(f\"Missing whitespace entries: {missing_whitespace} ({missing_whitespace / total * 100:.2f}%)\")"
   ],
   "id": "328f6c7115f4af55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing whitespace entries: 0 (0.00%)\n"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-27T11:20:00.272739Z",
     "start_time": "2025-04-27T11:20:00.266254Z"
    }
   },
   "cell_type": "code",
   "source": [
    "country_counts = df['country_normalized'].value_counts()\n",
    "country_counts"
   ],
   "id": "f2ff857824e9add1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "country_normalized\n",
       "Germany           341\n",
       "France            221\n",
       "Netherlands       148\n",
       "Italy             129\n",
       "Spain             111\n",
       "Sweden             76\n",
       "Ireland            74\n",
       "Poland             74\n",
       "Romania            53\n",
       "Denmark            43\n",
       "Austria            41\n",
       "Belgium            37\n",
       "Czech Republic     36\n",
       "Portugal           31\n",
       "Finland            31\n",
       "Bulgaria           23\n",
       "Slovenia           17\n",
       "Croatia            16\n",
       "Lithuania          16\n",
       "Latvia             15\n",
       "Greece             15\n",
       "Cyprus             11\n",
       "Hungary            10\n",
       "Luxembourg          9\n",
       "Slovakia            9\n",
       "Estonia             8\n",
       "Malta               5\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 57
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
