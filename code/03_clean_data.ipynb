{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:04.180140Z",
     "start_time": "2025-04-21T17:18:04.173849Z"
    }
   },
   "source": [
    "###\n",
    "# @Author             : Monserrat López\n",
    "# @Date               : 2025-03-17 \n",
    "# @Last Modified Date : 2025-04-21\n",
    "# @Description        : Cleaning and feature extraction pipeline for EU-based data centers\n",
    "###"
   ],
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:13:45.441847Z",
     "start_time": "2025-04-23T12:13:45.437924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import re\n",
    "import json\n",
    "import pandas as pd"
   ],
   "id": "55ba6c2ccc796f74",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:13:46.274720Z",
     "start_time": "2025-04-23T12:13:46.230429Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load scraped raw data from JSON cache\n",
    "with open(\"../cache/scrape_cache.json\", \"r\") as f:\n",
    "    data_raw = json.load(f)\n",
    "\n",
    "# Convert JSON dictionary to DataFrame\n",
    "df = pd.DataFrame.from_dict(data_raw, orient=\"index\").reset_index(drop=True)"
   ],
   "id": "aea2898ecfbe210d",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:13:47.299239Z",
     "start_time": "2025-04-23T12:13:47.227060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def split_operator_and_clean_address(address):\n",
    "    if pd.isna(address):\n",
    "        return pd.Series([None, None])\n",
    "    lines = address.strip().split('\\n')\n",
    "    operator = lines[0] if lines else None\n",
    "    address_lines = [line.strip() for line in lines[1:] if line.strip()]\n",
    "    clean_address = ', '.join(address_lines) if address_lines else None\n",
    "    return pd.Series([operator, clean_address])\n",
    "\n",
    "df[['operator_name', 'clean_address']] = df['Address'].apply(split_operator_and_clean_address)"
   ],
   "id": "f6110f478646648f",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:13:49.159685Z",
     "start_time": "2025-04-23T12:13:49.141635Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize country names\n",
    "df[\"Country\"] = df[\"Country\"].str.strip().str.lower()"
   ],
   "id": "23ae98cee95d0b27",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:14:09.371373Z",
     "start_time": "2025-04-23T12:14:09.325003Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Infer missing datacenter names from URLs\n",
    "def infer_name_from_url(url):\n",
    "    return url.strip(\"/\").split(\"/\")[-1].replace(\"-\", \"_\") if isinstance(url, str) else None\n",
    "\n",
    "df[\"DatacenterName\"] = df.apply(\n",
    "    lambda row: infer_name_from_url(row[\"URL\"]) if pd.isna(row[\"DatacenterName\"]) else row[\"DatacenterName\"],\n",
    "    axis=1\n",
    ")"
   ],
   "id": "190cde0458cd94f0",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:14:10.278153Z",
     "start_time": "2025-04-23T12:14:10.233951Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Filter out non-operational or planned facilities\n",
    "mask_exclude = df[\"Description\"].str.lower().str.contains(\n",
    "    \"archived listing|currently listed as: planned|under construction|this data center may not be available\", na=False\n",
    ")\n",
    "print(f\"Entries excluded due to construction/planning/archive: {mask_exclude.sum()}\")\n",
    "\n",
    "cleaned_df = df[~mask_exclude].copy()"
   ],
   "id": "cba61fb8c83229ab",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entries excluded due to construction/planning/archive: 180\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-23T12:14:19.337524Z",
     "start_time": "2025-04-23T12:14:19.327917Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize column names\n",
    "cleaned_df.columns = cleaned_df.columns.str.strip().str.replace(\" \", \"_\").str.lower()"
   ],
   "id": "96f6764cdedd0156",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Feature Extraction",
   "id": "376354d0cac7787f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:05.901012Z",
     "start_time": "2025-04-21T17:18:05.765455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract PUE from description\n",
    "def extract_pue(desc):\n",
    "    result = {\"pue_estimate\": None, \"has_pue\": False, \"pue_description\": None}\n",
    "    if pd.isna(desc): return pd.Series(result)\n",
    "    patterns = [\n",
    "        (r'(?:pue|power usage effectiveness).*?(?:between|from)\\s*(\\d\\.\\d+)\\s*(?:and|to)\\s*(\\d\\.\\d+)', lambda m: (float(m.group(1)) + float(m.group(2))) / 2),\n",
    "        (r'(?:pue|power usage effectiveness).*?(?:less than|as low as|under)\\s*(\\d\\.\\d+)', lambda m: float(m.group(1))),\n",
    "        (r'(?:pue|power usage effectiveness).*?(?:is|of|at|=)?\\s*(\\d\\.\\d+)', lambda m: float(m.group(1)))\n",
    "    ]\n",
    "    for pattern, func in patterns:\n",
    "        match = re.search(pattern, desc, re.IGNORECASE)\n",
    "        if match:\n",
    "            result[\"pue_estimate\"] = func(match)\n",
    "            result[\"has_pue\"] = True\n",
    "            result[\"pue_description\"] = match.group(0).strip()\n",
    "            break\n",
    "    return pd.Series(result)\n",
    "\n",
    "cleaned_df[[\"pue_estimate\", \"has_pue\", \"pue_description\"]] = cleaned_df[\"description\"].apply(extract_pue)\n",
    "cleaned_df[\"pue_estimate\"] = cleaned_df[\"pue_estimate\"].where(cleaned_df[\"pue_estimate\"].between(1.0, 2.5))\n"
   ],
   "id": "dd42d1b5a4770668",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:06.042500Z",
     "start_time": "2025-04-21T17:18:05.956064Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Extract specifications: power, whitespace, building size\n",
    "def extract_specs(spec):\n",
    "    result = {\"power_built_out_mw\": None, \"live_power_mw\": None, \"whitespace_sqm\": None, \"building_size_sqm\": None}\n",
    "    if pd.isna(spec): return pd.Series(result)\n",
    "    num_re = r'([\\d,]+(?:\\.\\d+)?)'\n",
    "    unit_re = r'(sq\\.?\\s?[mf]|m2|sqm|sqft|ft2|sq\\.?m\\.?|sq\\.?f\\.?)'\n",
    "    power = re.search(r'(?:built[-\\s]?out\\s+)?power\\s*[:\\-]?\\s*' + num_re + r'\\s*(mw|kw)', spec, re.IGNORECASE)\n",
    "    if power:\n",
    "        val, unit = float(power.group(1).replace(',', '')), power.group(2).lower()\n",
    "        result['power_built_out_mw'] = val if unit == 'mw' else val / 1000\n",
    "    live = re.search(r'(live|actual|available)\\s+power\\s*[:\\-]?\\s*' + num_re + r'\\s*(mw|kw)', spec, re.IGNORECASE)\n",
    "    if live:\n",
    "        val, unit = float(live.group(2).replace(',', '')), live.group(3).lower()\n",
    "        result['live_power_mw'] = val if unit == 'mw' else val / 1000\n",
    "    white = re.search(r'whitespace\\s+(?:built[-\\s]?out\\s+)?' + num_re + r'\\s*' + unit_re, spec, re.IGNORECASE)\n",
    "    if white:\n",
    "        val, unit = float(white.group(1).replace(',', '')), white.group(2).lower()\n",
    "        result['whitespace_sqm'] = val * 0.0929 if 'ft' in unit else val\n",
    "    build = re.search(r'building\\s+size\\s+' + num_re + r'\\s*' + unit_re, spec, re.IGNORECASE)\n",
    "    if build:\n",
    "        val, unit = float(build.group(1).replace(',', '')), build.group(2).lower()\n",
    "        result['building_size_sqm'] = val * 0.0929 if 'ft' in unit else val\n",
    "    return pd.Series(result)\n",
    "\n",
    "specs_df = cleaned_df[\"specs\"].apply(extract_specs)\n",
    "cleaned_df = pd.concat([cleaned_df, specs_df], axis=1)"
   ],
   "id": "6dd7fddb1a6c2fc0",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:06.146729Z",
     "start_time": "2025-04-21T17:18:06.134834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## Extract Tier from description\n",
    "def extract_tier(text):\n",
    "    if pd.isna(text): return None\n",
    "    match = re.search(r'tier\\s*(\\d|i{1,3}|iv)', text.lower())\n",
    "    roman_to_int = {'i': 1, 'ii': 2, 'iii': 3, 'iv': 4}\n",
    "    if match:\n",
    "        val = match.group(1).lower()\n",
    "        return int(val) if val.isdigit() else roman_to_int.get(val)\n",
    "    return None\n",
    "\n",
    "cleaned_df[\"tier_level\"] = cleaned_df[\"description\"].apply(extract_tier)\n",
    "cleaned_df[\"tier_level\"] = cleaned_df[\"tier_level\"].where(cleaned_df[\"tier_level\"].between(1, 4))"
   ],
   "id": "b8a8b09f9d23ae9",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:06.335556Z",
     "start_time": "2025-04-21T17:18:06.333258Z"
    }
   },
   "cell_type": "code",
   "source": "print(cleaned_df.columns.tolist())",
   "id": "b95eaf231c42ccff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['url', 'address', 'description', 'website', 'specs', 'status', 'country', 'city', 'datacentername', 'operator_name', 'clean_address', 'pue_estimate', 'has_pue', 'pue_description', 'power_built_out_mw', 'live_power_mw', 'whitespace_sqm', 'building_size_sqm', 'tier_level']\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:18:06.553780Z",
     "start_time": "2025-04-21T17:18:06.508088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Normalize country/city names and add ISO and region labels\n",
    "def normalize_location(df, country_col='country', city_col='city'):\n",
    "    EU27 = {\n",
    "        'austria': ('AT', 'AUT'), 'belgium': ('BE', 'BEL'), 'bulgaria': ('BG', 'BGR'),\n",
    "        'croatia': ('HR', 'HRV'), 'cyprus': ('CY', 'CYP'), 'czechia': ('CZ', 'CZE'),\n",
    "        'czech republic': ('CZ', 'CZE'), 'denmark': ('DK', 'DNK'), 'estonia': ('EE', 'EST'),\n",
    "        'finland': ('FI', 'FIN'), 'france': ('FR', 'FRA'), 'germany': ('DE', 'DEU'),\n",
    "        'deutschland': ('DE', 'DEU'), 'greece': ('GR', 'GRC'), 'hungary': ('HU', 'HUN'),\n",
    "        'ireland': ('IE', 'IRL'), 'italy': ('IT', 'ITA'), 'latvia': ('LV', 'LVA'),\n",
    "        'lithuania': ('LT', 'LTU'), 'luxembourg': ('LU', 'LUX'), 'malta': ('MT', 'MLT'),\n",
    "        'netherlands': ('NL', 'NLD'), 'poland': ('PL', 'POL'), 'portugal': ('PT', 'PRT'),\n",
    "        'romania': ('RO', 'ROU'), 'slovakia': ('SK', 'SVK'), 'slovenia': ('SI', 'SVN'),\n",
    "        'spain': ('ES', 'ESP'), 'españa': ('ES', 'ESP'), 'sweden': ('SE', 'SWE')\n",
    "    }\n",
    "    REGION_MAP = {\n",
    "        'Western Europe': ['FR', 'BE', 'LU', 'NL', 'DE', 'AT'],\n",
    "        'Northern Europe': ['SE', 'FI', 'DK', 'IE', 'EE', 'LV', 'LT'],\n",
    "        'Southern Europe': ['IT', 'ES', 'PT', 'GR', 'CY', 'MT'],\n",
    "        'Eastern Europe': ['PL', 'CZ', 'SK', 'HU', 'RO', 'BG', 'SI', 'HR']\n",
    "    }\n",
    "\n",
    "    def norm_country(c):\n",
    "        c = str(c).strip().lower().replace('-', ' ')\n",
    "        for name, (iso2, iso3) in EU27.items():\n",
    "            if name in c:\n",
    "                return (name.title(), iso2, iso3)\n",
    "        return ('Unknown', 'XX', 'XXX')\n",
    "\n",
    "    def norm_city(c):\n",
    "        if pd.isna(c): return \"\"\n",
    "        lowers = {'de', 'la', 'le', 'du', 'van', 'von', 'am', 'im', 'des', 'sur', 'el', 'di', 'del', 'della'}\n",
    "        return ' '.join([w.capitalize() if i == 0 or w not in lowers else w.lower() for i, w in\n",
    "                         enumerate(c.strip().replace('-', ' ').split())])\n",
    "\n",
    "    norm = df[country_col].apply(norm_country)\n",
    "    df['country_normalized'] = norm.apply(lambda x: x[0])\n",
    "    df['country_iso2'] = norm.apply(lambda x: x[1])\n",
    "    df['country_iso3'] = norm.apply(lambda x: x[2])\n",
    "    df['city_normalized'] = df[city_col].apply(norm_city)\n",
    "    df['region'] = df['country_iso2'].apply(lambda x: next((r for r, v in REGION_MAP.items() if x in v), 'Other'))\n",
    "    return df\n",
    "\n",
    "\n",
    "cleaned_df = normalize_location(cleaned_df, country_col=\"country\", city_col=\"city\")\n",
    "\n",
    "columns = [\n",
    "    \"url\", \"address\", \"clean_address\", \"country_iso2\", \"country_normalized\", \"city_normalized\",\n",
    "    \"website\", \"description\", \"specs\", \"region\", \"datacentername\", \"operator_name\", \"pue_estimate\",\n",
    "    \"power_built_out_mw\", \"live_power_mw\", \"whitespace_sqm\", \"building_size_sqm\", \"tier_level\"\n",
    "]\n",
    "\n",
    "cleaned_df = cleaned_df[columns].copy()\n",
    "cleaned_df.to_csv(\"../output/04eu_datacenters_cleaned_features.csv\", index=False, encoding=\"utf-8\")"
   ],
   "id": "b1ab6d4fbe3a1de2",
   "outputs": [],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-21T17:21:01.810053Z",
     "start_time": "2025-04-21T17:21:01.794057Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Saved cleaned_df data with {len(cleaned_df)} entries.\")",
   "id": "406a42ecf371883e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved cleaned_df data with 1615 entries.\n"
     ]
    }
   ],
   "execution_count": 51
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
